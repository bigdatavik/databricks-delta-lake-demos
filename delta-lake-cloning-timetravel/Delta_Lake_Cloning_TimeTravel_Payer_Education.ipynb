{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Delta Lake Table Cloning and Time Travel\n",
        "## An Interactive Education Demo for Healthcare Payers\n",
        "\n",
        "This notebook demonstrates Delta Lake's time travel, shallow clone, and deep clone features using PySpark SQL with managed tables only. \n",
        "\n",
        "**Story**: HealthFirst Insurance managing member claims data with compliance, dev/test, and disaster recovery needs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìã Table of Contents\n",
        "1. [Core Concepts](#core-concepts)\n",
        "2. [Configuration](#configuration)\n",
        "3. [Demo: Time Travel, Cloning & Recovery](#demo)\n",
        "4. [Real-World Healthcare Scenarios](#scenarios)\n",
        "5. [Best Practices & Key Takeaways](#best-practices)\n",
        "6. [Additional Resources](#resources)\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Core Concepts\n",
        "\n",
        "### What is Time Travel?\n",
        "Delta Lake's **time travel** allows you to query, restore, or audit previous versions of your data. Every modification creates a new version‚Äîperfect for:\n",
        "- **Compliance audits**: \"Show me the member claims as they existed on January 31st\"\n",
        "- **Rollback mistakes**: Undo accidental updates or deletes\n",
        "- **Data lineage**: Track how member status changed over time\n",
        "\n",
        "### What is a Shallow Clone?\n",
        "A **shallow clone** creates a new table that references the same underlying data files as the source table‚Äîno data is copied initially.\n",
        "\n",
        "**Healthcare Use Case**: Create a `member_claims_prod_dev` table for developers to test new adjudication logic without duplicating 500GB of production claims data.\n",
        "\n",
        "**Key Characteristic**: Metadata-only operation (fast and cheap), but dependent on source table.\n",
        "\n",
        "### What is a Deep Clone?\n",
        "A **deep clone** creates a completely independent copy of the table, including all data files.\n",
        "\n",
        "**Healthcare Use Case**: Create `member_claims_backup` for disaster recovery before a major migration or system upgrade.\n",
        "\n",
        "**Key Characteristic**: Full data copy (slower and more storage), but completely independent.\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Shallow Clone vs Deep Clone: Quick Comparison\n",
        "\n",
        "| Feature | **Shallow Clone** | **Deep Clone** |\n",
        "|---------|-------------------|----------------|\n",
        "| **Data Copied?** | ‚ùå No (references same files) | ‚úÖ Yes (full copy) |\n",
        "| **Speed** | ‚ö° Fast (metadata only) | üê¢ Slower (copies all data) |\n",
        "| **Storage Cost** | üí∞ Low (minimal) | üí∞üí∞ Higher (full duplication) |\n",
        "| **Independence** | ‚ö†Ô∏è Depends on source table | ‚úÖ Fully independent |\n",
        "| **Use After Source DROP** | ‚ö†Ô∏è Queryable but \"broken\" for 7 days* | ‚úÖ Works perfectly |\n",
        "| **Best For** | Dev/test, analytics sandboxes | DR, migration, archival |\n",
        "\n",
        "*Unity Catalog's 7-day retention allows queries but blocks metadata operations like VACUUM until source is restored.\n",
        "\n",
        "---\n",
        "\n",
        "## Configuration\n",
        "\n",
        "Update these values for your environment:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üè• Real-World Healthcare Payer Scenarios\n",
        "\n",
        "### 1. **Compliance Audit**\n",
        "**Challenge**: CMS requests claims data \"as it existed on March 31st for audit\"  \n",
        "**Solution**: Use time travel to query historical version\n",
        "```sql\n",
        "SELECT * FROM member_claims_prod TIMESTAMP AS OF '2025-03-31T23:59:59Z';\n",
        "```\n",
        "\n",
        "### 2. **Testing Claim Adjudication Rules**\n",
        "**Challenge**: New auto-adjudication logic needs testing without risking production data  \n",
        "**Solution**: Shallow clone production to dev environment\n",
        "```sql\n",
        "CREATE TABLE member_claims_dev SHALLOW CLONE member_claims_prod;\n",
        "-- Test new logic on shallow clone\n",
        "```\n",
        "\n",
        "### 3. **Pre-Migration Backup**\n",
        "**Challenge**: Migrating to new billing system; need 100% guaranteed backup  \n",
        "**Solution**: Deep clone before migration\n",
        "```sql\n",
        "CREATE TABLE member_claims_backup_2025_Q1 DEEP CLONE member_claims_prod;\n",
        "```\n",
        "\n",
        "### 4. **Accidental Data Deletion**\n",
        "**Challenge**: Developer accidentally runs `DELETE` on production table  \n",
        "**Solution**: Restore from previous version using time travel\n",
        "```sql\n",
        "RESTORE TABLE member_claims_prod TO VERSION AS OF 42;\n",
        "```\n",
        "\n",
        "### 5. **Multi-Region Disaster Recovery**\n",
        "**Challenge**: Need claims data available in secondary region for DR  \n",
        "**Solution**: Deep clone to DR region (with appropriate `LOCATION` for external tables)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using: quickstart_catalog_vkm_external.claims_analytics.member_claims_prod\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "catalog_name = \"quickstart_catalog_vkm_external\"\n",
        "schema_name = \"claims_analytics\"\n",
        "table_name = \"member_claims_prod\"\n",
        "\n",
        "spark.sql(f\"USE CATALOG {catalog_name}\")\n",
        "spark.sql(f\"USE SCHEMA {schema_name}\")\n",
        "\n",
        "print(f\"Using: {catalog_name}.{schema_name}.{table_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üè• Demo: Time Travel, Cloning & Recovery\n",
        "\n",
        "### Healthcare Payer Storyline\n",
        "\n",
        "**You are a Data Engineer at HealthFirst Insurance**, managing member claims data across multiple environments:\n",
        "\n",
        "- **Production Table** (`member_claims_prod`): Live claims data powering billing, reporting, and compliance\n",
        "- **Dev/Test Environment**: Developers need to test new claim adjudication rules without affecting production\n",
        "- **Disaster Recovery**: Compliance requires backups before major system changes\n",
        "- **Audit Requirements**: Regulators may ask \"What was the claim status on February 15th?\"\n",
        "\n",
        "This demo walks through a realistic workflow using Delta Lake's time travel and cloning features.\n",
        "\n",
        "---\n",
        "\n",
        "## Step 1: Create Production Claims Table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Created: member_claims_prod\n"
          ]
        }
      ],
      "source": [
        "# Create managed table\n",
        "spark.sql(f\"\"\"\n",
        "CREATE OR REPLACE TABLE {table_name} (\n",
        "  claim_id STRING,\n",
        "  member_id STRING,\n",
        "  provider_npi STRING,\n",
        "  diagnosis_code STRING,\n",
        "  claim_amount DECIMAL(10,2),\n",
        "  service_date DATE,\n",
        "  adjudication_status STRING,\n",
        "  last_updated TIMESTAMP\n",
        ") USING DELTA\n",
        "\"\"\")\n",
        "print(f\"‚úÖ Created: {table_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>claim_id</th>\n",
              "      <th>member_id</th>\n",
              "      <th>provider_npi</th>\n",
              "      <th>diagnosis_code</th>\n",
              "      <th>claim_amount</th>\n",
              "      <th>service_date</th>\n",
              "      <th>adjudication_status</th>\n",
              "      <th>last_updated</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CLM-001</td>\n",
              "      <td>MEM-78451</td>\n",
              "      <td>1234567890</td>\n",
              "      <td>E11.9</td>\n",
              "      <td>1250.00</td>\n",
              "      <td>2025-01-15</td>\n",
              "      <td>PENDING</td>\n",
              "      <td>2025-11-05 17:28:52.219467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CLM-002</td>\n",
              "      <td>MEM-78452</td>\n",
              "      <td>1234567891</td>\n",
              "      <td>I10</td>\n",
              "      <td>450.75</td>\n",
              "      <td>2025-01-16</td>\n",
              "      <td>APPROVED</td>\n",
              "      <td>2025-11-05 17:28:52.219467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CLM-003</td>\n",
              "      <td>MEM-78453</td>\n",
              "      <td>1234567892</td>\n",
              "      <td>J44.0</td>\n",
              "      <td>890.25</td>\n",
              "      <td>2025-01-17</td>\n",
              "      <td>PENDING</td>\n",
              "      <td>2025-11-05 17:28:52.219467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CLM-004</td>\n",
              "      <td>MEM-78454</td>\n",
              "      <td>1234567893</td>\n",
              "      <td>M79.3</td>\n",
              "      <td>325.50</td>\n",
              "      <td>2025-01-18</td>\n",
              "      <td>APPROVED</td>\n",
              "      <td>2025-11-05 17:28:52.219467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CLM-005</td>\n",
              "      <td>MEM-78455</td>\n",
              "      <td>1234567894</td>\n",
              "      <td>F41.1</td>\n",
              "      <td>675.00</td>\n",
              "      <td>2025-01-19</td>\n",
              "      <td>PENDING</td>\n",
              "      <td>2025-11-05 17:28:52.219467</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "DataFrame[claim_id: string, member_id: string, provider_npi: string, diagnosis_code: string, claim_amount: decimal(10,2), service_date: date, adjudication_status: string, last_updated: timestamp]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Insert data (Version 1)\n",
        "spark.sql(f\"\"\"\n",
        "INSERT INTO {table_name} VALUES\n",
        "  ('CLM-001', 'MEM-78451', '1234567890', 'E11.9', 1250.00, '2025-01-15', 'PENDING', current_timestamp()),\n",
        "  ('CLM-002', 'MEM-78452', '1234567891', 'I10', 450.75, '2025-01-16', 'APPROVED', current_timestamp()),\n",
        "  ('CLM-003', 'MEM-78453', '1234567892', 'J44.0', 890.25, '2025-01-17', 'PENDING', current_timestamp()),\n",
        "  ('CLM-004', 'MEM-78454', '1234567893', 'M79.3', 325.50, '2025-01-18', 'APPROVED', current_timestamp()),\n",
        "  ('CLM-005', 'MEM-78455', '1234567894', 'F41.1', 675.00, '2025-01-19', 'PENDING', current_timestamp())\n",
        "\"\"\")\n",
        "display(spark.sql(f\"SELECT * FROM {table_name} ORDER BY claim_id\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Create Version History\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Created 3 versions\n"
          ]
        }
      ],
      "source": [
        "# Update (Version 2) + Insert (Version 3)\n",
        "spark.sql(f\"UPDATE {table_name} SET adjudication_status = 'APPROVED' WHERE adjudication_status = 'PENDING'\")\n",
        "spark.sql(f\"\"\"\n",
        "INSERT INTO {table_name} VALUES\n",
        "  ('CLM-006', 'MEM-78456', '1234567895', 'K21.9', 550.00, '2025-01-20', 'PENDING', current_timestamp()),\n",
        "  ('CLM-007', 'MEM-78457', '1234567896', 'N18.3', 1875.50, '2025-01-21', 'UNDER_REVIEW', current_timestamp()),\n",
        "  ('CLM-008', 'MEM-78458', '1234567897', 'E78.5', 290.00, '2025-01-22', 'APPROVED', current_timestamp())\n",
        "\"\"\")\n",
        "print(\"‚úÖ Created 3 versions\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Time Travel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>version</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>userId</th>\n",
              "      <th>userName</th>\n",
              "      <th>operation</th>\n",
              "      <th>operationParameters</th>\n",
              "      <th>job</th>\n",
              "      <th>notebook</th>\n",
              "      <th>clusterId</th>\n",
              "      <th>readVersion</th>\n",
              "      <th>isolationLevel</th>\n",
              "      <th>isBlindAppend</th>\n",
              "      <th>operationMetrics</th>\n",
              "      <th>userMetadata</th>\n",
              "      <th>engineInfo</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>2025-11-05 17:29:02</td>\n",
              "      <td>8832166061032092</td>\n",
              "      <td>vik.malhotra@databricks.com</td>\n",
              "      <td>OPTIMIZE</td>\n",
              "      <td>{'clusterBy': '[]', 'zOrderBy': '[]', 'batchId': '0', 'predicate': '[]', 'auto': 'true'}</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>1105-171416-wzji5xnm-v2n</td>\n",
              "      <td>2.0</td>\n",
              "      <td>SnapshotIsolation</td>\n",
              "      <td>False</td>\n",
              "      <td>{'numRemovedFiles': '2', 'numRemovedBytes': '5684', 'p25FileSize': '3342', 'numDeletionVectorsRemoved': '1', 'conflictDetectionTimeMs': '88', 'minFileSize': '3342', 'p75FileSize': '3342', 'p50FileSize': '3342', 'numAddedBytes': '3342', 'numAddedFiles': '1', 'maxFileSize': '3342'}</td>\n",
              "      <td>None</td>\n",
              "      <td>Databricks-Runtime/17.2.x-photon-scala2.13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>2025-11-05 17:29:00</td>\n",
              "      <td>8832166061032092</td>\n",
              "      <td>vik.malhotra@databricks.com</td>\n",
              "      <td>WRITE</td>\n",
              "      <td>{'mode': 'Append', 'statsOnLoad': 'false', 'partitionBy': '[]'}</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>1105-171416-wzji5xnm-v2n</td>\n",
              "      <td>2.0</td>\n",
              "      <td>WriteSerializable</td>\n",
              "      <td>True</td>\n",
              "      <td>{'numFiles': '1', 'numOutputRows': '3', 'numOutputBytes': '2431'}</td>\n",
              "      <td>None</td>\n",
              "      <td>Databricks-Runtime/17.2.x-photon-scala2.13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2025-11-05 17:28:58</td>\n",
              "      <td>8832166061032092</td>\n",
              "      <td>vik.malhotra@databricks.com</td>\n",
              "      <td>UPDATE</td>\n",
              "      <td>{'predicate': '[\"(adjudication_status#17053 = PENDING)\"]'}</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>1105-171416-wzji5xnm-v2n</td>\n",
              "      <td>1.0</td>\n",
              "      <td>WriteSerializable</td>\n",
              "      <td>False</td>\n",
              "      <td>{'numRemovedFiles': '0', 'numRemovedBytes': '0', 'numCopiedRows': '0', 'numDeletionVectorsAdded': '1', 'executionTimeMs': '2018', 'numDeletionVectorsUpdated': '0', 'scanTimeMs': '802', 'numAddedFiles': '1', 'numUpdatedRows': '3', 'numDeletionVectorsRemoved': '0', 'numAddedChangeFiles': '0', 'numAddedBytes': '3197', 'rewriteTimeMs': '1215'}</td>\n",
              "      <td>None</td>\n",
              "      <td>Databricks-Runtime/17.2.x-photon-scala2.13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>2025-11-05 17:28:53</td>\n",
              "      <td>8832166061032092</td>\n",
              "      <td>vik.malhotra@databricks.com</td>\n",
              "      <td>WRITE</td>\n",
              "      <td>{'mode': 'Append', 'statsOnLoad': 'false', 'partitionBy': '[]'}</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>1105-171416-wzji5xnm-v2n</td>\n",
              "      <td>0.0</td>\n",
              "      <td>WriteSerializable</td>\n",
              "      <td>True</td>\n",
              "      <td>{'numFiles': '1', 'numOutputRows': '5', 'numOutputBytes': '2487'}</td>\n",
              "      <td>None</td>\n",
              "      <td>Databricks-Runtime/17.2.x-photon-scala2.13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>2025-11-05 17:28:51</td>\n",
              "      <td>8832166061032092</td>\n",
              "      <td>vik.malhotra@databricks.com</td>\n",
              "      <td>CREATE OR REPLACE TABLE</td>\n",
              "      <td>{'partitionBy': '[]', 'clusterBy': '[]', 'description': None, 'isManaged': 'true', 'properties': '{\"delta.parquet.compression.codec\":\"zstd\",\"delta.enableDeletionVectors\":\"true\",\"delta.enableRowTracking\":\"true\",\"delta.rowTracking.materializedRowCommitVersionColumnName\":\"_row-commit-version-col-16526301-bd61-4b6f-b78d-ae2e65bc6495\",\"delta.rowTracking.materializedRowIdColumnName\":\"_row-id-col-213e9716-37fc-4dff-a7c4-7b37db6cb507\"}', 'statsOnLoad': 'false'}</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>1105-171416-wzji5xnm-v2n</td>\n",
              "      <td>NaN</td>\n",
              "      <td>WriteSerializable</td>\n",
              "      <td>True</td>\n",
              "      <td>{}</td>\n",
              "      <td>None</td>\n",
              "      <td>Databricks-Runtime/17.2.x-photon-scala2.13</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "DataFrame[version: bigint, timestamp: timestamp, userId: string, userName: string, operation: string, operationParameters: map<string,string>, job: struct<jobId:string,jobName:string,jobRunId:string,runId:string,jobOwnerId:string,triggerType:string>, notebook: struct<notebookId:string>, clusterId: string, readVersion: bigint, isolationLevel: string, isBlindAppend: boolean, operationMetrics: map<string,string>, userMetadata: string, engineInfo: string]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Version 1 - Original data:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>claim_id</th>\n",
              "      <th>member_id</th>\n",
              "      <th>provider_npi</th>\n",
              "      <th>diagnosis_code</th>\n",
              "      <th>claim_amount</th>\n",
              "      <th>service_date</th>\n",
              "      <th>adjudication_status</th>\n",
              "      <th>last_updated</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CLM-001</td>\n",
              "      <td>MEM-78451</td>\n",
              "      <td>1234567890</td>\n",
              "      <td>E11.9</td>\n",
              "      <td>1250.00</td>\n",
              "      <td>2025-01-15</td>\n",
              "      <td>PENDING</td>\n",
              "      <td>2025-11-05 17:28:52.219467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CLM-002</td>\n",
              "      <td>MEM-78452</td>\n",
              "      <td>1234567891</td>\n",
              "      <td>I10</td>\n",
              "      <td>450.75</td>\n",
              "      <td>2025-01-16</td>\n",
              "      <td>APPROVED</td>\n",
              "      <td>2025-11-05 17:28:52.219467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CLM-003</td>\n",
              "      <td>MEM-78453</td>\n",
              "      <td>1234567892</td>\n",
              "      <td>J44.0</td>\n",
              "      <td>890.25</td>\n",
              "      <td>2025-01-17</td>\n",
              "      <td>PENDING</td>\n",
              "      <td>2025-11-05 17:28:52.219467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CLM-004</td>\n",
              "      <td>MEM-78454</td>\n",
              "      <td>1234567893</td>\n",
              "      <td>M79.3</td>\n",
              "      <td>325.50</td>\n",
              "      <td>2025-01-18</td>\n",
              "      <td>APPROVED</td>\n",
              "      <td>2025-11-05 17:28:52.219467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CLM-005</td>\n",
              "      <td>MEM-78455</td>\n",
              "      <td>1234567894</td>\n",
              "      <td>F41.1</td>\n",
              "      <td>675.00</td>\n",
              "      <td>2025-01-19</td>\n",
              "      <td>PENDING</td>\n",
              "      <td>2025-11-05 17:28:52.219467</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "DataFrame[claim_id: string, member_id: string, provider_npi: string, diagnosis_code: string, claim_amount: decimal(10,2), service_date: date, adjudication_status: string, last_updated: timestamp]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Time travel - query Version 1 (first INSERT)\n",
        "display(spark.sql(f\"DESCRIBE HISTORY {table_name}\"))\n",
        "print(\"\\nVersion 1 - Original data:\")\n",
        "display(spark.sql(f\"SELECT * FROM {table_name} VERSION AS OF 1 ORDER BY claim_id\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Shallow Clone (Dev/Test Environment)\n",
        "\n",
        "**Scenario**: Your development team needs to test new claim processing logic. Instead of copying 500GB of production data, create a shallow clone that references the same data files.\n",
        "\n",
        "**Benefits**:\n",
        "- ‚ö° Instant creation (metadata only)\n",
        "- üí∞ No additional storage cost initially\n",
        "- üîí Schema changes in dev won't affect production\n",
        "- üìä Developers work with real production data structure\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Created shallow clone: member_claims_prod_dev\n"
          ]
        }
      ],
      "source": [
        "# Shallow clone\n",
        "dev_table = f\"{table_name}_dev\"\n",
        "spark.sql(f\"DROP TABLE IF EXISTS {dev_table}\")\n",
        "spark.sql(f\"CREATE TABLE {dev_table} SHALLOW CLONE {table_name}\")\n",
        "print(f\"‚úÖ Created shallow clone: {dev_table}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Deep Clone (Disaster Recovery)\n",
        "\n",
        "**Scenario**: Before a major system upgrade (e.g., migrating to a new claims adjudication system), compliance requires a full backup that will remain accessible even if production is deleted.\n",
        "\n",
        "**Benefits**:\n",
        "- ‚úÖ Completely independent copy\n",
        "- üõ°Ô∏è Survives source table deletion\n",
        "- üì¶ Can be stored in different regions/accounts\n",
        "- üîÑ Perfect for disaster recovery and audits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Created deep clone: member_claims_prod_backup\n"
          ]
        }
      ],
      "source": [
        "# Deep clone  \n",
        "backup_table = f\"{table_name}_backup\"\n",
        "spark.sql(f\"DROP TABLE IF EXISTS {backup_table}\")\n",
        "spark.sql(f\"CREATE TABLE {backup_table} DEEP CLONE {table_name}\")\n",
        "print(f\"‚úÖ Created deep clone: {backup_table}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚ö†Ô∏è Step 6: DROP Test (Shallow vs Deep Clone)\n",
        "\n",
        "**This demonstrates Unity Catalog's 7-day safety net:**\n",
        "- Shallow clone: Queryable but \"broken\" for metadata operations\n",
        "- Deep clone: Completely unaffected\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö†Ô∏è Dropping member_claims_prod...\n",
            "‚úÖ Shallow clone: 8 rows (queryable but broken for VACUUM)\n",
            "‚úÖ Deep clone: 8 rows (completely independent!)\n"
          ]
        }
      ],
      "source": [
        "# DROP source table\n",
        "print(f\"‚ö†Ô∏è Dropping {table_name}...\")\n",
        "spark.sql(f\"DROP TABLE {table_name}\")\n",
        "\n",
        "# Test clones\n",
        "try:\n",
        "    count_dev = spark.sql(f\"SELECT COUNT(*) as count FROM {dev_table}\").collect()[0]['count']\n",
        "    print(f\"‚úÖ Shallow clone: {count_dev} rows (queryable but broken for VACUUM)\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Shallow clone failed: {e}\")\n",
        "\n",
        "try:\n",
        "    count_backup = spark.sql(f\"SELECT COUNT(*) as count FROM {backup_table}\").collect()[0]['count']\n",
        "    print(f\"‚úÖ Deep clone: {count_backup} rows (completely independent!)\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Deep clone failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: UNDROP - Recover Table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Using UNDROP to recover member_claims_prod...\n",
            "‚úÖ Table recovered! 8 rows restored\n",
            "‚úÖ Shallow clone now fully functional again\n"
          ]
        }
      ],
      "source": [
        "# UNDROP recovers table within 7-day window\n",
        "print(f\"üîÑ Using UNDROP to recover {table_name}...\")\n",
        "try:\n",
        "    spark.sql(f\"UNDROP TABLE {table_name}\")\n",
        "    count = spark.sql(f\"SELECT COUNT(*) FROM {table_name}\").collect()[0][0]\n",
        "    print(f\"‚úÖ Table recovered! {count} rows restored\")\n",
        "    print(f\"‚úÖ Shallow clone now fully functional again\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå UNDROP failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: VACUUM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üßπ VACUUM member_claims_prod_dev...\n",
            "‚úÖ VACUUM completed\n"
          ]
        }
      ],
      "source": [
        "# VACUUM (after UNDROP)\n",
        "print(f\"üßπ VACUUM {dev_table}...\")\n",
        "spark.sql(f\"VACUUM {dev_table} RETAIN 168 HOURS\")\n",
        "print(\"‚úÖ VACUUM completed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cleanup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üéØ Key Takeaways & Best Practices\n",
        "\n",
        "### When to Use Each Feature\n",
        "\n",
        "| Feature | Best Use Cases | Avoid When |\n",
        "|---------|---------------|------------|\n",
        "| **Time Travel** | Audits, rollbacks, debugging | Extremely old versions (check retention) |\n",
        "| **Shallow Clone** | Dev/test, analytics sandboxes | Need guaranteed independence from source |\n",
        "| **Deep Clone** | DR, migration, archival, cross-region | Budget-constrained (high storage cost) |\n",
        "\n",
        "### Best Practices for Healthcare Payers\n",
        "\n",
        "1. **üîí Shallow Clones for Development**\n",
        "   - Use shallow clones for dev/test environments to save storage costs\n",
        "   - Remember: Dependent on source table‚Äîdon't drop production without deep clone backup!\n",
        "\n",
        "2. **üõ°Ô∏è Deep Clones for Compliance**\n",
        "   - Create deep clones before major system changes (migrations, upgrades)\n",
        "   - Store quarterly deep clones for long-term audit requirements\n",
        "   - Consider cross-region deep clones for disaster recovery\n",
        "\n",
        "3. **üìÖ Time Travel for Audits**\n",
        "   - Leverage `VERSION AS OF` or `TIMESTAMP AS OF` for regulatory audits\n",
        "   - Set appropriate `delta.logRetentionDuration` (default 30 days) based on compliance needs\n",
        "   - Document version numbers for critical data snapshots\n",
        "\n",
        "4. **üßπ VACUUM with Caution**\n",
        "   - `VACUUM` removes old data files‚Äîdisables time travel to those versions\n",
        "   - Default retention: 7 days (168 hours)\n",
        "   - Never run `VACUUM RETAIN 0 HOURS` on production without backups\n",
        "\n",
        "5. **üîÑ UNDROP as Safety Net**\n",
        "   - Unity Catalog retains dropped tables for 7 days\n",
        "   - Use `UNDROP TABLE` to recover accidentally dropped tables\n",
        "   - Shallow clones remain queryable during this period but \"broken\" for metadata ops\n",
        "\n",
        "### Common Pitfalls to Avoid\n",
        "\n",
        "‚ùå **Don't** drop source tables with active shallow clones in production  \n",
        "‚ùå **Don't** assume shallow clones survive source deletion (they're queryable but \"broken\")  \n",
        "‚ùå **Don't** forget to deep clone before major migrations  \n",
        "‚ùå **Don't** run VACUUM too aggressively (kills time travel history)\n",
        "\n",
        "‚úÖ **Do** use shallow clones for cost-effective dev/test  \n",
        "‚úÖ **Do** use deep clones for disaster recovery  \n",
        "‚úÖ **Do** document version numbers for compliance snapshots  \n",
        "‚úÖ **Do** test UNDROP and backup procedures regularly\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìö Additional Resources\n",
        "\n",
        "### Official Azure Databricks Documentation\n",
        "\n",
        "#### Time Travel\n",
        "- [**Time Travel in Delta Lake Tables**](https://learn.microsoft.com/en-us/azure/databricks/delta/time-travel)  \n",
        "  Query and restore previous versions of your Delta tables\n",
        "\n",
        "- [**RESTORE TABLE Command**](https://docs.databricks.com/sql/language-manual/delta-restore.html)  \n",
        "  Restore a Delta table to an earlier version\n",
        "\n",
        "#### Cloning\n",
        "- [**Clone a Table on Azure Databricks**](https://learn.microsoft.com/en-us/azure/databricks/delta/clone)  \n",
        "  Comprehensive guide to deep and shallow clones\n",
        "\n",
        "- [**Shallow Clone for Unity Catalog Tables**](https://learn.microsoft.com/en-us/azure/databricks/delta/clone-unity-catalog)  \n",
        "  Unity Catalog-specific cloning behavior and limitations\n",
        "\n",
        "- [**CREATE TABLE CLONE SQL Syntax**](https://docs.databricks.com/sql/language-manual/delta-clone.html)  \n",
        "  Full SQL reference for CLONE commands\n",
        "\n",
        "#### Table Management\n",
        "- [**DROP TABLE Command**](https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-drop-table.html)  \n",
        "  DROP TABLE syntax and Unity Catalog retention policies\n",
        "\n",
        "- [**UNDROP TABLE Command**](https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-undrop-table.html)  \n",
        "  Recover dropped tables within 7-day retention window\n",
        "\n",
        "- [**VACUUM Command**](https://learn.microsoft.com/en-us/azure/databricks/delta/vacuum)  \n",
        "  Remove unused data files to reclaim storage\n",
        "\n",
        "#### Unity Catalog\n",
        "- [**Unity Catalog Best Practices**](https://learn.microsoft.com/en-us/azure/databricks/data-governance/unity-catalog/best-practices)  \n",
        "  Governance, security, and management recommendations\n",
        "\n",
        "- [**Managed Tables vs External Tables**](https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-create-table.html)  \n",
        "  Understanding table types in Unity Catalog\n",
        "\n",
        "### Delta Lake Resources\n",
        "- [**Delta Lake Official Documentation**](https://docs.delta.io/)  \n",
        "  Core Delta Lake concepts and features\n",
        "\n",
        "- [**Delta Lake Transaction Log**](https://docs.delta.io/latest/delta-intro.html#transaction-log)  \n",
        "  How Delta Lake tracks versions and enables time travel\n",
        "\n",
        "---\n",
        "\n",
        "## üéâ Congratulations!\n",
        "\n",
        "You've successfully learned how to:\n",
        "- ‚úÖ Use **time travel** to query and audit historical data\n",
        "- ‚úÖ Create **shallow clones** for cost-effective dev/test environments\n",
        "- ‚úÖ Create **deep clones** for disaster recovery and compliance\n",
        "- ‚úÖ Understand **Unity Catalog's 7-day retention** for dropped tables\n",
        "- ‚úÖ Use **UNDROP** to recover accidentally deleted tables\n",
        "- ‚úÖ Apply **VACUUM** to reclaim storage while preserving history\n",
        "\n",
        "### Next Steps\n",
        "1. Try this notebook in your own Databricks workspace\n",
        "2. Experiment with `RESTORE TABLE` for rollback scenarios\n",
        "3. Set up automated deep clones for critical production tables\n",
        "4. Configure `delta.logRetentionDuration` for your compliance needs\n",
        "\n",
        "**Questions?** Refer to the official documentation links above or reach out to your Databricks account team.\n",
        "\n",
        "---\n",
        "\n",
        "*This notebook demonstrates managed tables in Unity Catalog. For external tables with custom storage locations, consult the Databricks documentation for `LOCATION` clause usage.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Dropped: member_claims_prod\n",
            "‚úÖ Dropped: member_claims_prod_dev\n",
            "‚úÖ Dropped: member_claims_prod_backup\n",
            "\n",
            "‚úÖ Cleanup complete!\n"
          ]
        }
      ],
      "source": [
        "# Cleanup all demo tables\n",
        "for tbl in [table_name, dev_table, backup_table]:\n",
        "    try:\n",
        "        spark.sql(f\"DROP TABLE IF EXISTS {tbl}\")\n",
        "        print(f\"‚úÖ Dropped: {tbl}\")\n",
        "    except:\n",
        "        pass\n",
        "print(\"\\n‚úÖ Cleanup complete!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py312",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
